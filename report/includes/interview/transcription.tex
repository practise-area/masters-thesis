\chapter{Interview Transcription}\label{app:interview-transcription}
Following is the transcription from the interview conducted according to the interview plan in appendix \vref{app:interview-plan}. Asked questions which correspond to the interview questions in appendix~\ref{app:interview-questions} begin with a \textit{Q}, while other follow-up questions begin with \textit{FQ}. Answers to questions begin with an \textit{A}, while answers to follow-up questions begin with \textit{FA}. The following numbers each correspond to an interview question in appendix~\ref{app:interview-questions}.

For clarification purposes, each question and answer starts with either \textit{interviewer} or \textit{respondent}, denoting who said the following question or statement. Multiple follow-up questions are separated with numbers within brackets; see, for example, \textit{FQ.2(a) \{1\}} and \textit{FQ.2(a) \{2\}}. Also, for clarification, main questions areas (questions 1, 2, 3, and 4 in the interview questions) are separated with a horizontal line in the transcription.

Be aware that not all questions in appendix~\ref{app:interview-questions} are in the interview, as they are only used as a guideline. Also, some questions are answered indirectly before they are asked in the interview, so they do not need to be asked explicitly. Clarification questions from the respondent and answers to those from the interviewer have not been numbered but are included in the transcription. The respondent reviewed and accepted the original transcription.

Note that the whole interview, in its entirety, was held in Swedish.

\begin{otherlanguage}{swedish}
\begin{itemize}
  \item[Q.\ref{itm:int:role}] (Interviewer) Skulle du i grova drag kunna förklara din roll i projektet?
  \item[A.\ref{itm:int:role}] (Respondent) Jag började i det här projektet för snart två år sen och tanken var, när det började, att jag skulle hjälpa till med att sätta upp lite byggservrar och hjälpa till lite på backend-sidan. Men sedan gick tiden och det blev mer och mer jobb, så jag blev egentligen fullt sysselsatt med backend-programmering framförallt. Sen var det mycket prestandaförbättringar och sådant; prestandaproblem som jag har suttit och hållt på med - förbättrat prestanda helt enkelt. Mycket med lasttester och tagit det den vägen, kollat flaskhalsar och sådana saker. Så kan man väl sammanfatta det hela lite kortfattat.
  \item[Q.\ref{itm:int:role:a}] (Interviewer) Hur skulle du säga att testningen kommer in i det här arbetet? Är det bara prestandatester eller?
  \item[A.\ref{itm:int:role:a}] (Respondent) Nej det är det inte. För min del så har det varit mycket prestandatester nu på slutet, mycket lasttester, och så vidare. Det är mycket tester i projektet, alla typer av tester - integrationstester, gränssnittstester, regressiontester, och så vidare. Vi har många typer av tester som körs hela tiden. Dom körs på olika sätt, vissa vid commit - vid push till repot. Vissa körs med en periodicitet, så man bara kör natteligen eller med tre-timmars intervaller eller sådär. Så det beror lite på, men det körs kontinuerligt tester i alla fall, av olika typer.  
  \item[FQ.\ref{itm:int:role:a}] (Interviewer) Men om du skulle förklara hur du jobbar: vad är det för tester du är iblandad i och jobbar med? Är det alla typer av tester?
  \item[FA.\ref{itm:int:role:a}] (Respondent) Ja, i princip. Från enhetstester som man skriver så fort vi skriver någonting och så fortsätter det framåt. Har vi någon integration så är det smoke-test eller någon slags integrationstest. Och när vi kommer upp till gränssnittet så vill vi ha cucumber-tester, gränssnittstester, för det. Det är alla typer av tester.
  \item[Q.\ref{itm:int:role:c}] (Interviewer) Hur många är ni som arbetar med att vidareutveckla testerna?
  \item[] (Respondent) Alltså att skapa nya tester eller förbättra tester?
  \item[] (Interviewer) Ja, både och egentligen.
  \item[A.\ref{itm:int:role:c}] (Respondent) Alla deltagare i detta projektet som utvecklar jobbar ju med testerna. Så det har ju varit, under min tid i projektet, så har det varit fyra, fem, sex kanske personer som har skrivit tester regelbundet. Men sen har vi haft en sprint eller två sprintar egentligen under 2014 där vi har försökt förbättra prestandan på testerna. Alltså exekveringstiden på dem. Då har vi bland annat introducerat, vi började köra PhantomJS bland annat för att snabba upp cucumber-tester. Gick igenom och förbättrade testerna helt enkelt en del. Det var ju för att det är väldigt många tester och vissa tar för lång tid att köra, därför kör man dem med intervall istället för att köra dem kontinuerligt och så vidare. Sen så är vissa tester inte, integrationstester är inte värt att köra vid varje push till repot heller. Därför har man valt att sätta dem med en periodicitet istället för att man kör mer regelbundet då. Men, som sagt, under 2014 har vi arbetat. Jag har varit ganska inblandad i det att försöka snabba upp testerna, få dem mer stabila.  
  \item[FQ.\ref{itm:int:role:c}] (Interviewer) Men har det handlat om att ni skriver om en del tester då till exempel eller vad är det för typ av ändringar?
  \item[FA.\ref{itm:int:role:c}] (Respondent) Ja, precis. Delvis skriva om dem som har varit för contentspecifika - att man har tester som går direkt mot innehåll - till exempel, mer än att den ska kolla att det finns något på en viss plats kanske, det ska finnas,  det behöver inte vara exakt en specifik text, det ska bara finnas en knapp eller tillbakalänk eller logga ut knapp. Någon slags mer generell logik i testerna än att man testar mer specifikt. Sen kan det vara selektorer, till exempel XPath-uttryck, som är väldigt känsliga för förändringar i markup och så vidare, som har varit instabila. Men framför allt så har vi försökt testa nya ramverk, testa nya saker som förbättrar prestandan eller exekveringstid och så. Där har vi också tittat på SauceLabs bland annat för att ha testbänken någon annanstans, bara exekvera testerna mot en annan maskin. Man slipper en hel del problematik med setup och så vidare men och andra sidan förlorar man en hel del i hastighet - exekveringstid, som vi hade som största problem, så vi backade lite. Men vi har prövat många olika saker för att förbättra testerna då, öka kvalitén på testerna.
  \item[Q.\ref{itm:int:role:d}] (Interviewer) Arbetar ni efter någon speciell utvecklingsmetodik eller tankesätt?
  \item[A.\ref{itm:int:role:d}] (Respondent) Just ur testsynpunkt vet jag inte. Men rent utvecklingsmässigt så kör vi någon slags mix mellan Scrum och XP. Vi försöker jobba testdrivet så mycket som möjligt. 
  \item[] (Interviewer) Det var det jag tänkte fråga; om testerna har tagits fram som en del av detta tankesättet.
  \item[] (Respondent) Ja, precis. Vi jobbar så gott som testdrivet.
  
  \item[Q.\ref{itm:int:role:d:ii}] (Interviewer) Hur tycker du att det har fungerat?
  \item[A.\ref{itm:int:role:d:ii}] (Respondent) Det funkar jättebra. Det kan ha varit trösklar när man kommer in på totalt okänd mark, man har integrationer som ska byggas upp, då blir det mycket förarbete innan man kan komma in och koda direkt men i regel har det fungerat jättebra tycker jag. Man gör det man behöver göra. Det blir ingen onödig kod. Det är klart och tydligt vad koden gör och sådär.
  \item[Q.\ref{itm:int:role}$(*)$] (Interviewer) Jag tänkte summera lite kort bara. Du började för ungefär två år sedan, jobbade med lite småfix och byggservrar och sådant runt om och började sedan kika på prestanda och en del andra grejer. Ni har en hel del olika typer av tester i projektet: integrationstester, enhetstester, etc. Ni jobbar ungefär fyra, fem, sex personer med att utveckla tester regelbundet. Ni hade två sprintar där ni fokuserade speciellt på testningen. Det ni gjort framför allt är att utvärdera olika ramverk, PhantomJS bland annat, eller skrivit om testfall som var långsamma. Ni kör någon variant om Scrum och XP, ni försöker köra testdrivet så gott som det går. Stämmer det ungefär?
  \item[A.\ref{itm:int:role}$(*)$] (Respondent) Ja.

\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}
  
  \item[Q.\ref{itm:int:setup}] (Interviewer) Skulle du kunna förklara hur nuvarande test setup ser ut? Bara övergripande.
  \item[A.\ref{itm:int:setup}] (Respondent) Ja, absolut. Först och främst så har vi ju alla enhetstester då, dom körs vid varje commit och push till git-repot. Därefter så deployas koden, efter att alla testerna är okej, och när koden är deployad på testservern så körs alla cucumber-testerna igång och dom körs då i olika steg. Headless-PhantomJS-testerna körs igång direkt, sen har vi ett paket med cucumber-tester som testar en specifik del av applikationen som vi har byggt, som har med revidering att göra. Dom körs i webbläsaren, i en standard Firefox webbläsare. Så dem körs efter att headless testerna har körts. Efter det så körs integrationstester, som är selenium-tester. Det är nog dem testerna som körs.
    \item[FQ.\ref{itm:int:setup}] (Interviewer) Dom här cucumber-testerna, hur skiljer dom sig från integrationstesterna?
    \item[FA.\ref{itm:int:setup}] (Respondent) Integrationstesterna är äldre tester som är skriva i Selenium. I princip gör de samma sak, de testar specifika sidor som har med integrationspunkter att göra. Cucumber-testerna testar vylogik och annan sorts logik som inte har med olika integrationspunkter att göra, som inte hämtar in extern data utan bara lokalt i våra vyer och så.  
  \item[Q.\ref{itm:int:setup:a}] (Interviewer) Ser du några problem med hur test setupen ser ut idag?
  \item[A.\ref{itm:int:setup:a}] (Respondent) Ja, ett stort problem är ju att det tar tid att köra testerna. Så det är ju ett problem i sig för det gör ju att man dras ifrån att köra dem mer regelbundet. Det är ju ett problem som inte alltid är problem men som kan vara ett problem; att det döljer sig saker, som man inte känner till eftersom man har längre cykler så att säga, innan man kör dom. Det är väl framför allt det som är det största problemet. Det är inte avsaknad av tester utan vi har väldigt bra coverage om man skulle titta på det, utan det är just att exekveringstiden är så pass lång på delvis integrationstesterna men också på gränssnittstesterna, som tar lång tid, cucumber-testerna alltså.
  \item[FQ.\ref{itm:int:setup:a} \{1\}]\label{itm:fq2a:1} (Interviewer) Vilka typer av tester är det man drar sig för att köra?
  \item[FA.\ref{itm:int:setup:a} \{1\}] (Respondent) Det är framför allt cucumber-testerna, dem som körs i webbläsare, som tar tid.
  \item[FQ.\ref{itm:int:setup:a} \{2\}]\label{itm:fq2a:2} (Interviewer) Det här upplevs som ett problem för att det tar för lång tid, det är det som är det enda problemet?
  \item[FA.\ref{itm:int:setup:a} \{2\}] (Respondent) Ja, precis. Exekveringstiden om man kör alla testerna så tar det uppemot en timmes exekveringstid. Det är väldigt många tester som körs men det är lite för lång tid, helst skulle man vilja få ned det där till minuter istället för timmar.
  \item[Q.\ref{itm:int:setup:d}] (Interviewer) Brukar ni göra några förändringar i setupen eller är den ganska fixed?
  \item[A.\ref{itm:int:setup:d}] (Respondent) Ja, vi har ju gjort en del, som sagt, några sprintar tillbaka så har vi gjort en del förändringar, där vi introducerade PhantomJS bland annat. Sen uppdaterar vi hela tiden testbänken, vi har ju allting lokalt. Vi kör senaste Firefox-versionen, vi kör senaste WebDriver-versionerna för att hålla allting ajour med dem senaste versionerna i och med att vi stödjer ju den senaste versionen av Firefox hela tiden. Den testbänken uppdateras ju regelbundet och det medför ju också vissa problem med testerna. I normala fall brukar det inte vara så stora saker men när WebDriver-versionen som kör Firefox ändras eller när Firefox kommer i en ny version så får man uppdatera WebDrivern som i sin tur kanske inte gillar vissa selektorer eller inte gillar vissa settings, som vi har satt till exempel. Vi har vissa lågnivå settings på driven som vi har för att optimera exekveringstid och lite sådana saker. Det är sådant som kan hända, med dem uppdateringarna som görs då.
  \item[Q.\ref{itm:int:setup}$(*)$] (Interviewer) Jag tänkte bara kort summera där också. Som jag förstår det så var det enhetstester vid varje push, sen körs det en deploy till test där man kör integrationstester, både dem här cucumber-testerna och selenium-testerna. Det största problemet med test setupen idag är att det tar lång tid att köra testerna, det kan ta upp emot en timme att exekvera alla. Test setupen förändras i princip regelbundet med nya versioner och att det kan medföra vissa problem med till exempel att selektorer, som du nämnde. Stämmer det bra?
  \item[A.\ref{itm:int:setup}$(*)$] (Respondent) Ja.

\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

  \item[Q.\ref{itm:int:test}] (Interviewer) Kan du beskriva hur en utvecklare bör testa sina ändringar i projektet?
  \item[A.\ref{itm:int:test}] (Respondent) Förändrar man någon logik, så vill man ju gärna, så att säga, följa upp det med ett test. Gränsvärden, input som ska matas in, testa alla möjliga range på värden, testar en, testar två, testar flera, testar noll. Man testar det som förändringen ska testa, man ser om gränsvärden förändras eller någonting, att man tar med test på det. Om det var någonting som var fel från början, att man testar det som var fel. 
  
  \item[Q.\ref{itm:int:test:a}] (Interviewer) Tror du det finns någonting som skulle kunna förbättras i processen - hur en utvecklare bör testa?
  \item[] (Respondent) Från den processen som vi har idag eller?
  \item[] (Interviewer) Ja, från så som du ser på att man bör testa.
  \item[A.\ref{itm:int:test:a}] (Respondent) Det kanske är att man bör tänka på det som vi har problem med idag; exekveringstid, som är det största problemet egentligen och att man försöker skriva tester som går mot specifika känsliga element eller något sådant, känsligt content. Så att man får en högre kvalité på testerna på det sättet. Testa så lite, fast så mycket som möjligt samtidigt. Att man inte har överlappande tester och sådant där det kostar exekveringstid men det inte ger någonting mer egentligen. Det är ett problem i sig för att när testsviten blir större och större, så är det väldigt lätt att det blir överlappande tester, det är också ett litet problem.
  
  \item[Q.\ref{itm:int:test:c}] (Interviewer) Vilka typer av tester är det som tar för lång tid?
  \item[A.\ref{itm:int:test:c}] (Respondent) Det är cucumber-testerna, som tar överlägset lång tid.
  
   \item[Q.\ref{itm:int:test:d}] (Interviewer) Varför är det ett problem att det tar för lång tid?
  \item[A.\ref{itm:int:test:d}] (Respondent) Det är det ju för att det gör att man dras ifrån att köra dem mer regelbundet. Det gör också att det kan dölja sig fel nu, så att säga. Dom testerna kan ju avslöja fel, som man normalt sett inte ser förrän dem körs vid den regelbundna tidpunkten, som dem är inställda att köras på. 
  
  \item[Q.\ref{itm:int:test:e}] (Interviewer) Skulle du säga att det är en allmän uppfattning bland utvecklarna att testerna tar för lång tid?
  \item[A.\ref{itm:int:test:e}] (Respondent) Ja, dem här testerna tycker nog dem flesta tar för lång tid. Vi kör ju alltid gränssnittstesterna lokalt på maskinerna när vi håller på och utvecklar men vi kör ju inte alla testerna utan vi kör ju dem vi skriver nya, så att säga, dem kör man på sin egen maskin innan man commitar och så där. Man kör ju inte samtliga tester när man har utvecklat något nytt, så det har ju hänt att det har kommit upp saker som blivit fel i cucumber-testerna när vi har pushat någonting till repot, som man har märkt efteråt när dem har körts skedulerat. Det är ju ett problem. Då måste vi vänta tills att cucumber-testerna är klara innan, till exempel, vi ska bygga ett releasepaket, så måste vi köra alla cucumber-testerna innan vi kan bygga paketet - för att vi ska vara säkra att ingenting har gått sönder i gränssnittet. Så det får ju sådana konsekvenser också, att det tar lite längre tid innan man kan få ut paket och sådana saker.
  
  \item[Q.\ref{itm:int:test}$(*)$] (Interviewer) Kanon. Jag tänkte summera lite där också. Som en utvecklare bör man i princip testa alla logiska förändringar som man gjort. När man skriver tester i arbetsprocessen skulle man kunna tänka på saker så som exekveringstid, att det tar lång tid, eller att man testar specifika element och att man få nått sätt försöker undvika överlappande tester. Det är framför allt cucumber-testerna som tar för lång tid och problemet att det tar för lång tid är att man drar sig för att köra testerna och det kan leda till att man inte upptäcker dolda fel som testerna skulle kunna upptäcka. Det verkar vara en allmän uppfattning att det är just cucumber-testerna som tar lång tid. Stämmer det ungefär?
  \item[A.\ref{itm:int:test}$(*)$] (Respondent) Ja, det stämmer.
  
\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

  \item[Q.\ref{itm:int:improve}] (Interviewer) Nu har vi redan varit inne lite på det men skulle du kunna förklara hur ni arbetar med att förbättra era testsviter?
  \item[A.\ref{itm:int:improve}] (Respondent) När vi gör sprintplaneringarna så brukar vi ju diskutera lite grann just kring testerna. Vi har ju tagit fram acceptanskriterier när vi har sprintplanering, så att acceptanskriterierna ligger som underlag för till exempel cucumber-testerna. Vi använder Capybara, till exempel, för att översätta acceptanskriterierna mot tester. På så sätt är det ganska bra, vi får nästan  en ett till ett mappning direkt från acceptanskriter till att bli ett cucumber-test. Men sen har vi alltid under designsessioner under sprinten där vi diskuterar just om det är någon integration som vi behöver ha upp för integrationstester eller om det är något specifikt som det behöver läggas extra koll på då förutom det normala då. Det är väl egentligen det. Sen jobbar vi med parprogrammering ganska mycket, då är det också bra att man sitter och liksom har fyra ögon på det man gör hela tiden, så att det är riktigt bra faktiskt.

  \item[FQ.\ref{itm:int:improve}] (Interviewer) Om man tänker existerande testfall, har ni gjort något annat än det du nämnde tidigare med sprintarna där ni fokuserade på testfall? Har ni gjort något annat för att förbättra testfallen?
  \item[FA.\ref{itm:int:improve}] (Respondent) Nej, inte förutom det här att vi tittat på andra ramverk.

  \item[Q.\ref{itm:int:improve:a}] (Interviewer) Har du några liksom egna tankar eller idéer på förbättringar som skulle kunna göras runt testsviterna?
  \item[A.\ref{itm:int:improve:a}] (Respondent) Ja, det är ju det här med exkveringstid åter igen, som är ett problem fortfarande. Där i samband med att vi tittade på SauceLabs, så var vi inne på att försöka parallellisera cucumber-teserna - körningen av dem. Det är någonting som jag absolut tror kan öka på prestandan relativt mycket, där man helt enkelt kör flera trådar för testerna, så man exekverar dem parallellt. Det har vi inte prövat men jag är hyffsat säker på att det borde kunna gå att snabba upp testerna rätt rejält genom att göra det, för dom ska inte ha några beroenden mellan varandra, så det ska bara vara att köra i princip så mycket som maskinerna klarar av liksom.
  
  \item[Q.\ref{itm:int:improve:b}] (Interviewer) Känner du till några verktyg eller metoder som skulle kunna användas?
  \item[] (Respondent) För att?
  \item[] (Interviewer) För att förbättra testerna.
  \item[A.\ref{itm:int:improve:b}] (Respondent) Egentligen inte verktyg. Det finns ju många sådana här, inget specifikt, men det finns ju många verkyg som kan hjälpa till och titta på just det här med överlappande tester och man kan titta på coverage - är det någon del i systemet som inte testas. Så det finns ju många sådana verktyg, som man kan titta på och använda sig av. Det är inget specifikt sådär som jag har funderat på. Men det finns massor av sådana nyttiga program som man kan köra igenom och analysera sitt projekt. Men annars så finns det ju massa, i Ruby där vi kör alla dom här cucumber-testerna, så finns det ju sätt att parallellisera testerna. Bland annat så har ju SauceLabs något eget gem som man kan köra med också. Vi har kikat lite grann men vi har inte testkört någonting men jag har gjort lite research.
  
  \item[Q.\ref{itm:int:improve:c}] (Interviewer) Har ni arbetat någonting med att göra urval av testfall för att det liksom tar för lång tid att köra alla tester?
  \item[A.\ref{itm:int:improve:c}] (Respondent) Ja, det har vi ju redan gjort. Vi har gjort en uppsplittning av cucumber-testerna, där vi kör vissa i headless-läge och sen har vi flyttat ut vissa saker som tar väldigt lång tid - det är dom här gränssnittstesterna eller, förlåt, eller precis, jo, vi har ett gränssnittsflöde i applikationen där det är väldigt mycket spårval, det är en massa gafflingar och förgreningar och sådant där. Så att dom tar väldigt lång tid att köra för det är så pass många fall som man måste ta sig igenom, så dom har vi lyft ut, och dom kör vi med en högre periodictet då, med längre tidsintervall då. Så vi har gjort en viss gallring utav testerna, dom som tar extremt lång tid, så att dom inte förstör för dem andra testerna. Och sen har vi dom som går väldigt snabbt kör vi då tillsammans med PhantomJS. 
  
  \item[Q.\ref{itm:int:improve:c:iii}] (Interviewer) Hur tror du att liksom urval kan förbättra testningen? 
  \item[] (Respondent) Urval förbättra testningen, ja...
  \item[] (Interviewer) Eller vad tjänar man på att göra ett urval?
  \item[A.\ref{itm:int:improve:c:iii}] (Respondent) Det man tjänar är att man får fram fel snabbare genom att man kör de tester som går snabbt oftare, så får man snabbare cykler, snabbare feedback loop och det är ju jätteviktigt om man vill höja kvalitén liksom.
  
  \item[Q.\ref{itm:int:improve:d}] (Interviewer) Yes. Har ni arbetat något med att rangordna eller prioritera testfall?
  \item[A.\ref{itm:int:improve:d}] (Respondent) Nej, det tror jag inte. Det har vi inte gjort.
  \item[FQ.\ref{itm:int:improve:d}] (Interviewer) Det görs ingen prioritering liksom överlag vilka testfall som borde köras tidigare i testsviten? 
  \item[FA.\ref{itm:int:improve:d}] (Respondent) Nej, det tror jag inte.
  
  \item[Q.\ref{itm:int:improve:d:ii}] (Interviewer) Om man skulle göra någon typ av prioritering av testfallen, vad tror du liksom skulle vara vettigt att basera en sådan prioritering på?
  \item[A.\ref{itm:int:improve:d:ii}] (Respondent) Den borde baseras på, ja det är ju flera faktorer. Det är egentligen ur kundperspektiv den som har högst affärsvärde eller vad man ska säga. Men sen är det också hur frekvent någonting används och hur stor sannolikhet det nånting har att... hur känsligt för förändringar och så där kanske, som man måste väga in också.
  
  \item[Q.\ref{itm:int:improve:d:iii}] (Interviewer) Hur tror du att prioritering skulle kunna förbättra testningen?
  \item[A.\ref{itm:int:improve:d:iii}] (Respondent) Ja, dom viktiga delarna skulle man kunna få något högre kvalitét på, om man har prioriterat så att säga. Jag kan tänka mig att det skulle få lite, att man höjer kvalitén i systemet generellt sett liksom. 
  \item[FQ.\ref{itm:int:improve:d:iii} \{1\}] (Interviewer) Hur tänker du alltså att den kvalitén höjs?
  \item[FA.\ref{itm:int:improve:d:iii} \{1\}] (Respondent) Att man helt enkelt förbättrar exekveringstiden på dem testerna som är av högst värde eller så att dem körs mer regelbundet eller att man skriver om vissa eller i och med att man prioriterar dem ger dem mer tid helt enkelt.
  \item[FQ.\ref{itm:int:improve:d:iii} \{2\}] (Interviewer) Så med prioritering menar du att dom får mer tid att köra eller?
  \item[FA.\ref{itm:int:improve:d:iii} \{2\}] (Respondent) Ja, eller att man går igenom och förändrar dom eller skriver om dom, förbättrar dom helt enkelt. 
  \item[FQ.\ref{itm:int:improve:d:iii} \{3\}] (Interviewer) Om vi tänker prioritering som att vi har en testsvit och så ger vi en prioritet till dem olika testfallen; dom med högre prioritet skulle köras tidigare i testsviten. Om man tänker en sån typ av prioritering. Hur tror du att någon sådan typ av prioritering skulle kunna förbättra testningen?
  \item[FA.\ref{itm:int:improve:d:iii} \{3\}] (Respondent) Jag vet inte vad det skulle få för effekt. Tveksamt faktiskt, jag vet inte riktigt. Hur som haver, så kör vi alla tester oavsett så om man byter ordning på testerna hur dom körs, så är det mycket möjligt att vi skulle få att man skulle få, ja, kanske... jag vet faktiskt inte. 
  
  \item[Q.\ref{itm:int:improve:d}$(*)$] (Interviewer) Jättebra. Jag tänkte summera lite där också. Ni har acceptanskriterier som ni kopplar till tester via Capybara och så har ni designsessioner där ni försöker förbättra testerna på liksom särskilda integrationspunkter till exempel, som är känsliga. Ni har arbetat lite med SauceLabs, som du nämnde innan, och att försöka parallellisera exekveringen och du var lite osäker på om det fanns beroenden, du trodde inte det fanns några beroenden, mellan testfallen. Man kan använda lite verktyg som kollar efter överlappande tester och till exempel coverage och ni har gjort lite urval av testfall, till exempel lyfta ut gui-testningen och kör den med längre periodictet för att den tar lång tid. Och du trodde att urval av testfall kunde göra så att man fick feedback snabbare, så att man fick snabbare feedback cykler och ni har inte direkt arbetat med att göra någon prioritering av testfall och du var lite osäker på hur en sådan prioritering skulle kunna hjälpa till. Stämmer det någorlunda?
  \item[A.\ref{itm:int:improve:d}$(*)$] (Respondent) Yes.
\end{itemize}
\end{otherlanguage}